{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59776f90-6824-4a70-9ea4-648730061c11",
   "metadata": {},
   "source": [
    "모델 만들기 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2718f3-a9bf-4822-aaf4-643f98de8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7c02f75-6df4-41ea-9636-012e1bf95617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e58971-4045-4b6f-8cc7-cef9102c3af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('data', download=True, transform=v2.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77f04e8-b415-4f30-aa85-a006dc19cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32)\n",
    "#배치 사이즈 32개씩으로 처리하겠다. 고 했으므로\n",
    "#X_train의 배치크기가 32, 로 시작하는거고 y_label도 32로 나오는거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac68f5d-6105-44e4-a80b-933fa48b4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "374a0b8a-e8d3-4852-afcc-e4beef0e2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5,self).__init__()\n",
    "        self.features = nn.Sequential( nn.Conv2d(3, 6, 5, 1, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Conv2d(6, 16, 5, 1, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2),\n",
    "                        nn.Conv2d(16, 126, 5, 1, padding='same'),\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(2))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.classifier = nn.Sequential( nn.Linear(2016 ,128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(128, 64),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(64, 10),        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3a2437f-21da-4d8e-b526-78ed87d3bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(16, 126, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2016, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4f604fa-a0a9-4b0a-bc1a-a0bb6d15bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#이 크로스엔트로피가 softmax와 중첩되어 이중적용되니 이상한 결과가 나오므로 \n",
    "#softmax를 모델에서 없앴음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27b846e1-d806-4225-a39e-6dc35cbdf9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2016])\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for X_train, y_label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(X_train)\n",
    "        loss = loss_fn(outputs, y_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f70c170-0289-42d7-905d-8880ada1af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1134])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0472,  0.0824, -0.0550,  0.0361, -0.0065, -0.1271,  0.0483,  0.0392,\n",
       "          0.0143, -0.0361],\n",
       "        [-0.0459,  0.0823, -0.0536,  0.0381, -0.0059, -0.1280,  0.0483,  0.0382,\n",
       "          0.0128, -0.0348],\n",
       "        [-0.0472,  0.0802, -0.0520,  0.0362, -0.0074, -0.1274,  0.0474,  0.0392,\n",
       "          0.0155, -0.0336],\n",
       "        [-0.0461,  0.0815, -0.0547,  0.0384, -0.0051, -0.1255,  0.0475,  0.0378,\n",
       "          0.0144, -0.0344],\n",
       "        [-0.0468,  0.0823, -0.0548,  0.0367, -0.0050, -0.1284,  0.0476,  0.0371,\n",
       "          0.0141, -0.0323],\n",
       "        [-0.0457,  0.0806, -0.0526,  0.0402, -0.0054, -0.1254,  0.0475,  0.0379,\n",
       "          0.0139, -0.0337],\n",
       "        [-0.0468,  0.0828, -0.0518,  0.0391, -0.0066, -0.1261,  0.0477,  0.0383,\n",
       "          0.0146, -0.0360],\n",
       "        [-0.0471,  0.0819, -0.0531,  0.0390, -0.0044, -0.1266,  0.0494,  0.0378,\n",
       "          0.0125, -0.0346],\n",
       "        [-0.0468,  0.0818, -0.0549,  0.0396, -0.0045, -0.1294,  0.0479,  0.0383,\n",
       "          0.0130, -0.0341],\n",
       "        [-0.0468,  0.0825, -0.0528,  0.0377, -0.0056, -0.1251,  0.0483,  0.0358,\n",
       "          0.0147, -0.0348],\n",
       "        [-0.0471,  0.0818, -0.0544,  0.0379, -0.0072, -0.1269,  0.0490,  0.0366,\n",
       "          0.0131, -0.0336],\n",
       "        [-0.0470,  0.0818, -0.0544,  0.0379, -0.0043, -0.1273,  0.0492,  0.0379,\n",
       "          0.0137, -0.0347],\n",
       "        [-0.0469,  0.0823, -0.0533,  0.0370, -0.0069, -0.1284,  0.0492,  0.0368,\n",
       "          0.0135, -0.0321],\n",
       "        [-0.0468,  0.0818, -0.0538,  0.0375, -0.0065, -0.1261,  0.0468,  0.0378,\n",
       "          0.0144, -0.0338],\n",
       "        [-0.0464,  0.0802, -0.0530,  0.0360, -0.0071, -0.1284,  0.0479,  0.0383,\n",
       "          0.0147, -0.0330],\n",
       "        [-0.0473,  0.0848, -0.0554,  0.0349, -0.0078, -0.1273,  0.0467,  0.0385,\n",
       "          0.0131, -0.0339],\n",
       "        [-0.0482,  0.0837, -0.0549,  0.0385, -0.0067, -0.1259,  0.0473,  0.0384,\n",
       "          0.0150, -0.0334],\n",
       "        [-0.0468,  0.0804, -0.0533,  0.0379, -0.0049, -0.1281,  0.0500,  0.0389,\n",
       "          0.0129, -0.0333],\n",
       "        [-0.0470,  0.0801, -0.0535,  0.0379, -0.0061, -0.1276,  0.0477,  0.0388,\n",
       "          0.0126, -0.0318],\n",
       "        [-0.0465,  0.0826, -0.0553,  0.0386, -0.0066, -0.1282,  0.0481,  0.0373,\n",
       "          0.0141, -0.0343],\n",
       "        [-0.0459,  0.0822, -0.0530,  0.0376, -0.0069, -0.1261,  0.0483,  0.0372,\n",
       "          0.0129, -0.0343],\n",
       "        [-0.0470,  0.0826, -0.0520,  0.0371, -0.0046, -0.1254,  0.0484,  0.0392,\n",
       "          0.0140, -0.0351],\n",
       "        [-0.0452,  0.0811, -0.0529,  0.0397, -0.0081, -0.1258,  0.0480,  0.0374,\n",
       "          0.0143, -0.0350],\n",
       "        [-0.0451,  0.0815, -0.0537,  0.0370, -0.0074, -0.1269,  0.0489,  0.0384,\n",
       "          0.0144, -0.0341],\n",
       "        [-0.0468,  0.0815, -0.0537,  0.0367, -0.0085, -0.1277,  0.0493,  0.0366,\n",
       "          0.0141, -0.0329],\n",
       "        [-0.0463,  0.0803, -0.0525,  0.0382, -0.0053, -0.1274,  0.0485,  0.0371,\n",
       "          0.0142, -0.0325],\n",
       "        [-0.0467,  0.0807, -0.0550,  0.0390, -0.0047, -0.1283,  0.0486,  0.0361,\n",
       "          0.0143, -0.0337],\n",
       "        [-0.0481,  0.0828, -0.0527,  0.0372, -0.0065, -0.1273,  0.0471,  0.0371,\n",
       "          0.0144, -0.0355],\n",
       "        [-0.0476,  0.0821, -0.0539,  0.0377, -0.0044, -0.1268,  0.0493,  0.0370,\n",
       "          0.0136, -0.0340],\n",
       "        [-0.0463,  0.0801, -0.0537,  0.0386, -0.0053, -0.1270,  0.0497,  0.0361,\n",
       "          0.0144, -0.0334],\n",
       "        [-0.0468,  0.0806, -0.0557,  0.0376, -0.0071, -0.1287,  0.0474,  0.0375,\n",
       "          0.0123, -0.0330],\n",
       "        [-0.0443,  0.0797, -0.0528,  0.0402, -0.0067, -0.1267,  0.0497,  0.0377,\n",
       "          0.0141, -0.0359]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(32, 3, 28, 28)\n",
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3388a522-06f3-4870-8428-185d04edcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53c0b7eb-736e-437e-8108-fcc19f059f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [10:00<00:00, 284095.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST('data', download=True, transform=v2.ToTensor() )\n",
    "dataset = FashionMNIST('data', download=True, transform=v2.ToTensor())\n",
    "dataset = CIFAR10('data', download=True, transform=v2.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d13fe18f-45ec-46b6-af6d-5529046b4299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87289c1d-6802-445e-a128-87038e68998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#배치 단위로 사용하려면 DataLoader가 필요함! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77b07bce-9581-4e3e-a32e-dc46e43fe0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b903770e-d314-4ea7-95d5-c19ed45fd77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2016])\n"
     ]
    }
   ],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "   model(X_train)\n",
    "   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "905ce325-c342-43bd-a954-6ee79ead499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss() #는 softmax를 포함하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b986fa0-367d-4473-86f8-aff0f6a9f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) #model의 파라미터를 다 갱신할것들 weight 갱신할것들을 adam에게 주는거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb7110a1-3517-47b4-998e-fa7d76fb2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_train, y_label in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = loss_fn(outputs, y_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0098c-55d3-4d14-a6a3-12fd512203ee",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = LeNet5()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa4f2d-360e-489f-a191-fcefd8f25652",
   "metadata": {},
   "source": [
    "이미지 불러오고 신경망 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac073d99-e87e-4e90-aca3-3b505b704469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    "    def __init__(self):\n",
    "        self.data = list(range(100, 200))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.rand(1, 28, 28), 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ff4585b-5246-4e3c-9a73-2f743c31e3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), int)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "data, label = dataset[0]\n",
    "data.shape, type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "675eb7c3-016b-4c33-8a07-8a1585064a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      " - pytorch\n",
      " - anaconda\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\user\\anaconda3\\envs\\torch-book\n",
      "\n",
      "  added / updated specs:\n",
      "    - opencv\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/win-64::certifi-2024.7.4-py~ --> conda-forge/noarch::certifi-2024.7.4-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages: ...working... done\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge opencv -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26db346e-8112-4c3f-84a4-3fbe072001ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms #이미지 변환 전처리 라이브러리\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a15f4b74-9ab0-471d-bf18-06731866a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b8162e92-a874-42e6-a042-25c3619cc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "class MyDataset():\n",
    "    def __init__(self, root='./080289/chap06/data/dogs-vs-cats'):\n",
    "        self.root = root\n",
    "        self.image_paths = []\n",
    "        self.__classes = {'Cat': 0, 'Dog': 1}\n",
    "        self.labels = []\n",
    "\n",
    "        for dname in os.listdir(self.root):\n",
    "            print(type(dname))\n",
    "            print(os.path.isdir(Path(root, dname)))\n",
    "            new_path = Path(root, dname)\n",
    "            if os.path.isdir(new_path):\n",
    "                for file in os.listdir(new_path):\n",
    "                    #print(file)\n",
    "                    self.image_paths.append(str(Path(new_path, file)))\n",
    "                    self.labels.append(self.__classes[dname])\n",
    "        \n",
    "        self.transform = v2.Compose([ \n",
    "             v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "             v2.ToTensor()])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        #print(path)\n",
    "        image = Image.open(path)\n",
    "        #print(self.transform(image).shape)\n",
    "        return self.transform(image) , self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7ddef739-c5de-40ec-a2f3-c3ce8dd10f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "True\n",
      "<class 'str'>\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), 0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset('./080289/chap06/data/dogs-vs-cats')\n",
    "data, label = dataset[0]\n",
    "data.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b3aa0fd7-71e6-4006-8202-8c6d483078ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for X_train, y_label in data_loader:\n",
    "    print(X_train.shape, y_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31389a4f-4ca6-4b14-a356-b358781ea815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(3, 16, 5, 1, 0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2),\n",
    "                                    nn.Conv2d(16, 32, 5, 1, 0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(2))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(32*53*53, 512),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(512, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
